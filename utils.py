"""
å·¥å…·æ¨¡å—
æä¾›å›¾ç‰‡å¤„ç†ã€æ–‡ä»¶æ“ä½œã€ç³»ç»Ÿæ£€æµ‹ç­‰å·¥å…·å‡½æ•°
"""

import os
import sys
import platform
import subprocess
import logging
from pathlib import Path
from typing import Tuple, Optional, List
from PIL import Image

# å¯é€‰ä¾èµ–
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

def setup_logging(log_dir: str = "logs") -> logging.Logger:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_path = Path(log_dir)
    log_path.mkdir(exist_ok=True)
    
    log_file = log_path / "sd_generator.log"
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)

def get_system_info() -> dict:
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    info = {
        "platform": platform.platform(),
        "system": platform.system(),
        "machine": platform.machine(),
        "processor": platform.processor(),
        "python_version": platform.python_version(),
    }

    # ç³»ç»Ÿèµ„æºä¿¡æ¯ï¼ˆéœ€è¦psutilï¼‰
    if PSUTIL_AVAILABLE:
        info["cpu_count"] = psutil.cpu_count()
        info["memory_total"] = psutil.virtual_memory().total
        info["memory_available"] = psutil.virtual_memory().available
    else:
        info["cpu_count"] = os.cpu_count() or 1
        info["memory_total"] = "æœªçŸ¥"
        info["memory_available"] = "æœªçŸ¥"

    # GPUä¿¡æ¯ï¼ˆéœ€è¦torchï¼‰
    if TORCH_AVAILABLE and torch.cuda.is_available():
        info["cuda_available"] = True
        info["cuda_version"] = torch.version.cuda
        info["gpu_count"] = torch.cuda.device_count()
        info["gpu_names"] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]
        info["gpu_memory"] = [torch.cuda.get_device_properties(i).total_memory for i in range(torch.cuda.device_count())]
    else:
        info["cuda_available"] = False

    return info

def check_dependencies() -> Tuple[bool, List[str]]:
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    required_packages = [
        "torch",
        "diffusers",
        "transformers",
        "PIL",
        "numpy"
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    return len(missing_packages) == 0, missing_packages

def get_optimal_device() -> str:
    """è·å–æœ€ä¼˜è®¾å¤‡ - ä¼˜åŒ–CUDAé€‰æ‹©ç­–ç•¥ï¼Œä¼˜å…ˆä½¿ç”¨GPU"""
    if not TORCH_AVAILABLE:
        print("PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨CPUæ¨¡å¼")
        return "cpu"

    if not torch.cuda.is_available():
        print("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
        print("æç¤º: å¦‚æœæ‚¨æœ‰NVIDIA GPUï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥è·å¾—æ›´å¥½æ€§èƒ½")
        return "cpu"

    # è·å–GPUä¿¡æ¯
    gpu_count = torch.cuda.device_count()
    if gpu_count == 0:
        print("æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨CPUæ¨¡å¼")
        return "cpu"

    # é€‰æ‹©æœ€ä½³GPUï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªï¼‰
    device_id = 0
    gpu_props = torch.cuda.get_device_properties(device_id)
    gpu_memory_gb = gpu_props.total_memory / (1024**3)
    gpu_name = gpu_props.name

    print(f"ğŸ® æ£€æµ‹åˆ°GPU: {gpu_name}")
    print(f"ğŸ’¾ GPUå†…å­˜: {gpu_memory_gb:.1f}GB")

    # æ›´ç§¯æçš„CUDAé€‰æ‹©ç­–ç•¥ - åªè¦æœ‰GPUå°±å°è¯•ä½¿ç”¨
    if gpu_memory_gb >= 6.0:
        print(f"âœ… GPUå†…å­˜å……è¶³({gpu_memory_gb:.1f}GB >= 6GB)ï¼Œä½¿ç”¨CUDAåŠ é€Ÿï¼Œæ€§èƒ½æœ€ä½³")
        return "cuda"
    elif gpu_memory_gb >= 4.0:
        print(f"âœ… GPUå†…å­˜è‰¯å¥½({gpu_memory_gb:.1f}GB >= 4GB)ï¼Œä½¿ç”¨CUDAåŠ é€Ÿ")
        return "cuda"
    elif gpu_memory_gb >= 2.0:
        print(f"âš ï¸ GPUå†…å­˜è¾ƒå°‘({gpu_memory_gb:.1f}GB)ï¼Œä½¿ç”¨CUDAä½†å°†å¯ç”¨ä½æ˜¾å­˜ä¼˜åŒ–")
        return "cuda"
    elif gpu_memory_gb >= 1.0:
        print(f"âš ï¸ GPUå†…å­˜å¾ˆå°‘({gpu_memory_gb:.1f}GB)ï¼Œä½¿ç”¨CUDAä½†æ€§èƒ½å¯èƒ½å—é™")
        return "cuda"
    else:
        print(f"âŒ GPUå†…å­˜ä¸è¶³({gpu_memory_gb:.1f}GB < 1GB)ï¼Œå»ºè®®ä½¿ç”¨CPUæ¨¡å¼")
        return "cpu"

def format_memory_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–å†…å­˜å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} PB"

def save_image_with_metadata(image: Image.Image, filepath: str, metadata: dict = None):
    """ä¿å­˜å›¾ç‰‡å¹¶æ·»åŠ å…ƒæ•°æ®"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    
    # æ·»åŠ å…ƒæ•°æ®åˆ°å›¾ç‰‡
    if metadata:
        from PIL.PngImagePlugin import PngInfo
        png_info = PngInfo()
        for key, value in metadata.items():
            png_info.add_text(key, str(value))
        image.save(filepath, pnginfo=png_info)
    else:
        image.save(filepath)

def resize_image_for_display(image: Image.Image, max_size: Tuple[int, int]) -> Image.Image:
    """è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥é€‚åº”æ˜¾ç¤º"""
    image.thumbnail(max_size, Image.Resampling.LANCZOS)
    return image

def validate_prompt(prompt: str) -> Tuple[bool, str]:
    """éªŒè¯æç¤ºè¯"""
    if not prompt or not prompt.strip():
        return False, "æç¤ºè¯ä¸èƒ½ä¸ºç©º"
    
    if len(prompt) > 1000:
        return False, "æç¤ºè¯è¿‡é•¿ï¼Œè¯·æ§åˆ¶åœ¨1000å­—ç¬¦ä»¥å†…"
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸å½“å†…å®¹çš„åŸºæœ¬å…³é”®è¯
    inappropriate_keywords = ["nude", "naked", "nsfw", "porn", "sex"]
    prompt_lower = prompt.lower()
    for keyword in inappropriate_keywords:
        if keyword in prompt_lower:
            return False, f"æç¤ºè¯åŒ…å«ä¸å½“å†…å®¹: {keyword}"
    
    return True, ""

def generate_filename(prompt: str, seed: int, timestamp: str = None) -> str:
    """ç”Ÿæˆæ–‡ä»¶å"""
    import re
    from datetime import datetime
    
    if timestamp is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # æ¸…ç†æç¤ºè¯ï¼Œåªä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼
    clean_prompt = re.sub(r'[^\w\s-]', '', prompt)
    clean_prompt = re.sub(r'\s+', '_', clean_prompt.strip())
    
    # é™åˆ¶é•¿åº¦
    if len(clean_prompt) > 50:
        clean_prompt = clean_prompt[:50]
    
    filename = f"{timestamp}_{clean_prompt}_seed{seed}.png"
    return filename

def check_disk_space(path: str, required_gb: float = 1.0) -> bool:
    """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
    if not PSUTIL_AVAILABLE:
        return True  # å¦‚æœpsutilä¸å¯ç”¨ï¼Œå‡è®¾ç©ºé—´è¶³å¤Ÿ

    try:
        free_space = psutil.disk_usage(path).free
        required_bytes = required_gb * 1024**3
        return free_space >= required_bytes
    except:
        return True  # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œå‡è®¾ç©ºé—´è¶³å¤Ÿ

def install_package(package_name: str) -> bool:
    """å®‰è£…PythonåŒ…"""
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package_name])
        return True
    except subprocess.CalledProcessError:
        return False

def create_desktop_shortcut(app_path: str, shortcut_name: str = "SDå›¾ç‰‡ç”Ÿæˆå™¨"):
    """åˆ›å»ºæ¡Œé¢å¿«æ·æ–¹å¼ï¼ˆä»…Windowsï¼‰"""
    if platform.system() != "Windows":
        return False
    
    try:
        import winshell
        from win32com.client import Dispatch
        
        desktop = winshell.desktop()
        shortcut_path = os.path.join(desktop, f"{shortcut_name}.lnk")
        
        shell = Dispatch('WScript.Shell')
        shortcut = shell.CreateShortCut(shortcut_path)
        shortcut.Targetpath = sys.executable
        shortcut.Arguments = f'"{app_path}"'
        shortcut.WorkingDirectory = os.path.dirname(app_path)
        shortcut.IconLocation = sys.executable
        shortcut.save()
        
        return True
    except ImportError:
        print("éœ€è¦å®‰è£… pywin32 å’Œ winshell æ¥åˆ›å»ºå¿«æ·æ–¹å¼")
        return False
    except Exception as e:
        print(f"åˆ›å»ºå¿«æ·æ–¹å¼å¤±è´¥: {e}")
        return False

def get_cuda_optimization_settings(device: str = None) -> dict:
    """è·å–CUDAä¼˜åŒ–è®¾ç½®å»ºè®®"""
    settings = {
        "attention_slicing": True,
        "cpu_offload": False,
        "low_vram_mode": False,
        "use_fp16": False,
        "use_bf16": False,
        "sequential_cpu_offload": False
    }
    
    if not TORCH_AVAILABLE or not torch.cuda.is_available():
        return settings
    
    try:
        gpu_props = torch.cuda.get_device_properties(0)
        gpu_memory_gb = gpu_props.total_memory / (1024**3)
        gpu_name = gpu_props.name.lower()
        
        # æ ¹æ®GPUå†…å­˜è°ƒæ•´è®¾ç½®
        if gpu_memory_gb >= 12:
            # é«˜ç«¯GPU (12GB+)
            settings.update({
                "attention_slicing": False,  # å…³é—­æ³¨æ„åŠ›åˆ‡ç‰‡ä»¥è·å¾—æ›´å¥½æ€§èƒ½
                "cpu_offload": False,
                "low_vram_mode": False,
                "use_bf16": True,  # ä½¿ç”¨bfloat16è·å¾—æ›´å¥½æ€§èƒ½
                "sequential_cpu_offload": False
            })
        elif gpu_memory_gb >= 8:
            # ä¸­é«˜ç«¯GPU (8-12GB)
            settings.update({
                "attention_slicing": True,
                "cpu_offload": False,
                "low_vram_mode": False,
                "use_bf16": True,
                "sequential_cpu_offload": False
            })
        elif gpu_memory_gb >= 6:
            # ä¸­ç«¯GPU (6-8GB)
            settings.update({
                "attention_slicing": True,
                "cpu_offload": True,  # å¯ç”¨CPUå¸è½½
                "low_vram_mode": False,
                "use_fp16": True,  # ä½¿ç”¨fp16èŠ‚çœå†…å­˜
                "sequential_cpu_offload": False
            })
        elif gpu_memory_gb >= 4:
            # ä¸­ä½ç«¯GPU (4-6GB)
            settings.update({
                "attention_slicing": True,
                "cpu_offload": True,
                "low_vram_mode": True,  # å¯ç”¨ä½æ˜¾å­˜æ¨¡å¼
                "use_fp16": True,
                "sequential_cpu_offload": False
            })
        else:
            # ä½ç«¯GPU (<4GB)
            settings.update({
                "attention_slicing": True,
                "cpu_offload": True,
                "low_vram_mode": True,
                "use_fp16": True,
                "sequential_cpu_offload": True  # å¯ç”¨é¡ºåºCPUå¸è½½
            })
        
        # ç‰¹å®šGPUä¼˜åŒ–
        if "rtx" in gpu_name and ("30" in gpu_name or "40" in gpu_name):
            # RTX 30/40ç³»åˆ—æ”¯æŒæ›´å¥½çš„bf16
            settings["use_bf16"] = True
            settings["use_fp16"] = False
        
        return settings
        
    except Exception as e:
        print(f"è·å–CUDAä¼˜åŒ–è®¾ç½®æ—¶å‡ºé”™: {e}")
        return settings

# åˆå§‹åŒ–æ—¥å¿—
logger = setup_logging()
